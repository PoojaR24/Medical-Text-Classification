{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pooja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pooja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy as sp\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open test file and read its lines\n",
    "with open(\"train.dat\", 'r') as fh:\n",
    "    lines1 = fh.readlines()\n",
    "with open(\"test.dat\", 'r') as fh:\n",
    "    lines2 = fh.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting the scentences into words\n",
    "trn = [l.split() for l in lines1]\n",
    "tst = [l.split() for l in lines2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining the class labels from documents\n",
    "labels = []\n",
    "words = []\n",
    "for i in range (0,len(trn)):\n",
    "    labels.append(trn[i][0])\n",
    "    words.append(trn[i][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>[Catheterization, laboratory, events, and, hos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>[Renal, abscess, in, children., Three, cases, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[Hyperplastic, polyps, seen, at, sigmoidoscopy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>[Subclavian, artery, to, innominate, vein, fis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Effect, of, local, inhibition, of, gamma-amin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Labels                                             Tokens\n",
       "0      4  [Catheterization, laboratory, events, and, hos...\n",
       "1      5  [Renal, abscess, in, children., Three, cases, ...\n",
       "2      2  [Hyperplastic, polyps, seen, at, sigmoidoscopy...\n",
       "3      5  [Subclavian, artery, to, innominate, vein, fis...\n",
       "4      4  [Effect, of, local, inhibition, of, gamma-amin..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualising the class labels for each parah\n",
    "table = pd.DataFrame()\n",
    "\n",
    "table['Labels'] = labels[:]\n",
    "table['Tokens'] = words[:]\n",
    "\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the train and test data in a single document\n",
    "\n",
    "for i in range(0,len(tst)):\n",
    "    words.append(tst[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPROCESSING ##\n",
    "\n",
    "# converting words to lowercase and remove digits\n",
    "def filterInput(words):\n",
    "    new_words = []\n",
    "    for i in words:\n",
    "        new = []\n",
    "        for word in i:\n",
    "            new.append(word.lower())\n",
    "            for char in word:\n",
    "                if(not char.isalpha()):\n",
    "                    new.remove(word.lower())\n",
    "                    break\n",
    "        new_words.append(new)\n",
    "    \n",
    "    return new_words\n",
    "\n",
    "# Remove stop words\n",
    "def stop_words_remover(words):\n",
    "    eng_words = set(stopwords.words('english'))\n",
    "    new_words = []\n",
    "    for i in words:\n",
    "        new = []  \n",
    "        for word in i:\n",
    "            if word not in eng_words:\n",
    "                new.append(word)\n",
    "        new_words.append(new)\n",
    "            \n",
    "    return new_words\n",
    "\n",
    "# Remove puntuations\n",
    "# \\s refers to the whitespace characters (which includes [ \\t\\n\\r\\f\\v])\n",
    "# \\w includes most characters that can be part of a word in any language, as well as numbers and the underscore\n",
    "def punct_remover(words):\n",
    "    new_docs = []\n",
    "    for i in words:\n",
    "        new_words = []  \n",
    "        for word in i:\n",
    "            new = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new != '':\n",
    "                new_words.append(new)\n",
    "        new_docs.append(new_words)\n",
    "            \n",
    "    return new_docs\n",
    "\n",
    "# Remove words shorter than the minlen\n",
    "def filterLen(docs, minlen):\n",
    "    r\"\"\" filter out terms that are too short. \n",
    "    docs is a list of lists, each inner list is a document represented as a list of words\n",
    "    minlen is the minimum length of the word to keep\n",
    "    \"\"\"\n",
    "    return [ [t for t in d if len(t) >= minlen ] for d in docs ]\n",
    "\n",
    "# lemmatize and keep meaningful ('verb') words\n",
    "def lemmatize(docs):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        lemmas = []  \n",
    "        for word in doc:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "            lemmas.append(lemma)\n",
    "        new_docs.append(lemmas)\n",
    "            \n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "words_1 = stop_words_remover(words)\n",
    "words_2 = punct_remover(words_1)\n",
    "words_3 = filterLen(words_2,4)\n",
    "words_4 = lemmatize(words_3)\n",
    "words_5 = filterInput(words_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale matrix and normalize its rows\n",
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 28880, ncols 55021, nnz 1916480]\n",
      "None\n",
      " [nrows 28880, ncols 55021, nnz 1916480]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Generating csr matrix\n",
    "M = build_matrix(words_5)\n",
    "print(csr_info(M))\n",
    "N = csr_idf(M, copy=True)\n",
    "Words = csr_l2normalize(N, copy=True)\n",
    "M = build_matrix(words_5)\n",
    "print(csr_info(Words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating Test and Train data \n",
    "Train_words = Words[0:14438]\n",
    "Test_words = Words[14438:]\n",
    "Train_label = labels[0:14438]\n",
    "Test_label = labels[14438:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28880, 55021)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cosine Similarity\n",
    "def cosine_sim(i,trainword):\n",
    "    prod = i.dot(trainword.T)\n",
    "    csim = list(zip(prod.indices, prod.data))\n",
    "    return csim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classification\n",
    "def Classification(i,train_word,train_label,k):\n",
    "    \n",
    "    cosim = cosine_sim(i,train_word)\n",
    "    if len(cosim) == 0:\n",
    "        \n",
    "        if np.random.rand() > 0.5: \n",
    "            return '+'\n",
    "        else:\n",
    "            return '-'\n",
    "    cosim.sort(key=lambda i: i[1], reverse = True)\n",
    "    count = Counter(train_label[c[0]] for c in cosim[:k]).most_common(2)\n",
    "    if len(count) < 2 or count[0][1] >count[1][1]:\n",
    "        return count[0][0]\n",
    "    count = defaultdict(float)\n",
    "    for c in cosim[:k]:\n",
    "        count[train_label[c[0]]] += c[1]\n",
    "    return sorted(count.items(), key=lambda i: i[1], reverse = True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Predict = []\n",
    "file = open(\"Prediction3.dat\",\"w+\")\n",
    "for i in Test_words:\n",
    "    predicted_class = Classification(i,Train_words, Train_label,39)\n",
    "    file.write(str(predicted_class) + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
